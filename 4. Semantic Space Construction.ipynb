{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Semantic Space Construction\n",
    "\n",
    "In this NB, I will apply the new `heads` edge feature to extract head nouns from their phrase and record their co-occurring verbs, subjects, objects, and coordinates. Each of these relationships is assigned a weight. Those co-occurrences are then placed into a matrix. Then I assign an associational measure to the counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 3.2.2\n",
      "Api reference : https://github.com/Dans-labs/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/Dans-labs/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Example data  : https://github.com/Dans-labs/text-fabric-data\n",
      "\n",
      "116 features found and 0 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.01s B book                 from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B chapter              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B verse                from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.07s B function             from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.11s B lex                  from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.10s B vs                   from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.10s B language             from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.10s B pdp                  from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.07s B freq_lex             from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B gloss                from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.01s B domain               from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.01s B voc_lex_utf8         from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.22s B heads                from /Users/cody/github/semantics/tf/c\n",
      "   |     0.00s Feature overview: 109 for nodes; 6 for edges; 1 configs; 7 computed\n",
      "  4.75s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tf.fabric import Fabric\n",
    "\n",
    "TF = Fabric(locations='~/github', modules=['etcbc/bhsa/tf/c', 'semantics/tf/c'])\n",
    "api = TF.load('''\n",
    "                book chapter verse\n",
    "                function lex vs language\n",
    "                pdp freq_lex gloss domain\n",
    "                voc_lex_utf8\n",
    "                heads\n",
    "              ''')\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather and Count Noun Relations\n",
    "\n",
    "Now I will gather nouns from the Hebrew Bible and count syntactic co-occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure weights\n",
    "path_weights = {'Subj': {'Pred': 1,\n",
    "                         'Objc': 1\n",
    "                        },\n",
    "                'Objc': {\n",
    "                         'Pred': 1,\n",
    "                         'Subj': 1\n",
    "                        },\n",
    "                'coor': 1\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616 nouns\n",
      "3656 cooccurrences\n"
     ]
    }
   ],
   "source": [
    "cooccurrences = collections.defaultdict(lambda: collections.Counter()) # noun counts here\n",
    "\n",
    "# Subj/Objc Counts\n",
    "for phrase in F.otype.s('phrase'):\n",
    "    \n",
    "    # skip non-Hebrew sections\n",
    "    language = F.language.v(L.d(phrase, 'word')[0]) \n",
    "    if language != 'Hebrew':\n",
    "        continue\n",
    "    \n",
    "    # skip non subject/object phrases\n",
    "    function = F.function.v(phrase)\n",
    "    if function not in {'Subj', 'Objc'}:\n",
    "        continue\n",
    "    \n",
    "    # get head nouns\n",
    "    nouns = set(F.lex.v(w) for w in E.heads.f(phrase)) # count lexemes only once\n",
    "    if not nouns:\n",
    "        continue\n",
    "    \n",
    "    # restrict on frequency\n",
    "    freq = [F.freq_lex.v(L.u(w, 'lex')[0]) for w in E.heads.f(phrase)]\n",
    "    if min(freq) < 25:\n",
    "        continue\n",
    "\n",
    "    # restrict on proper names\n",
    "    types = set(F.pdp.v(w) for w in E.heads.f(phrase))\n",
    "    if {'nmpr'} & types:\n",
    "        continue\n",
    "\n",
    "    # restrict on domain\n",
    "#     if F.domain.v(L.u(phrase, 'clause')[0]) != 'N':\n",
    "#         continue\n",
    "    \n",
    "    # gather contextual data\n",
    "    clause = L.u(phrase, 'clause')[0]\n",
    "    good_paths = path_weights[function]\n",
    "    paths = [phrase for phrase in L.d(clause, 'phrase')\n",
    "                if F.function.v(phrase) in good_paths.keys()\n",
    "            ]\n",
    "    \n",
    "    # make the counts\n",
    "    for path in paths:\n",
    "        \n",
    "        pfunct = F.function.v(path)\n",
    "        weight = good_paths[pfunct]\n",
    "        \n",
    "        # count for verb\n",
    "        if pfunct == 'Pred':\n",
    "            verb = [w for w in L.d(path, 'word') if F.pdp.v(w) == 'verb'][0]\n",
    "            verb_lex = F.lex.v(verb)\n",
    "            verb_stem = F.vs.v(verb)\n",
    "            verb_basis = function + '.' + verb_lex + '.' + verb_stem # with function name added\n",
    "            if verb and F.lex.v(verb) not in {'HJH['}: # omit \"to be\" verbs, others?\n",
    "                for noun in nouns:\n",
    "                    cooccurrences[noun][verb_basis] += 1\n",
    "        \n",
    "        # count for subj/obj\n",
    "        else:\n",
    "            conouns = E.heads.f(path)\n",
    "            cnoun_bases = set(function + '.' + F.lex.v(w) + f'.{pfunct}' for w in conouns) # with function name added\n",
    "            counts = dict((basis, weight) for basis in cnoun_bases)\n",
    "            if counts:\n",
    "                for noun in nouns:\n",
    "                    cooccurrences[noun].update(counts)\n",
    "    \n",
    "    # count coordinates\n",
    "    for noun in nouns:\n",
    "        for cnoun in nouns:\n",
    "            if cnoun != noun:\n",
    "                cnoun_basis = 'coor.'+cnoun # with coordinate function name\n",
    "                cooccurrences[noun][cnoun_basis] += path_weights['coor']\n",
    "\n",
    "cooccurrences = pd.DataFrame(cooccurrences).fillna(0)                \n",
    "\n",
    "print(len(cooccurrences.columns), 'nouns')\n",
    "print(len(cooccurrences.index), 'cooccurrences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Association Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_log(number):\n",
    "    '''\n",
    "    Evaluate for zero before applying log function.\n",
    "    '''\n",
    "    if number == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return math.log(number)\n",
    "    \n",
    "def loglikelihood(k, l, m, n, log):\n",
    "    '''\n",
    "    Returns the log-likelihood when the supplied elements are given.\n",
    "    '''\n",
    "    \n",
    "    p1 = (k*log(k)) + (l*log(l)) + (m*log(m)) + (n*log(n))        \n",
    "    p2 = ((k+l)*log(k+l)) - ((k+m)*log(k+m))\n",
    "    p3 = ((l+n)*log(l+n)) - ((m+n)*log(m+n))\n",
    "    p4 = ((k+l+m+n))*log(k+l+m+n)\n",
    "    llikelihood = 2*(p1-p2-p3+p4)\n",
    "    return llikelihood\n",
    "\n",
    "def apply_loglikelihood(comatrix):\n",
    "    \n",
    "    '''\n",
    "    Adjusts values in a cooccurrence matrix using log-likelihood. \n",
    "    Requires a cooccurrence matrix.\n",
    "    '''\n",
    "    new_matrix = comatrix.copy()\n",
    "    i = 0 \n",
    "    indent(reset=True)\n",
    "    info('beginning calculations...')\n",
    "    indent(1, reset=True)\n",
    "    for target in comatrix.columns:\n",
    "        for basis in comatrix.index:\n",
    "            k = comatrix[target][basis]\n",
    "            \n",
    "            if not k:\n",
    "                i += 1\n",
    "                if i % 500000 == 0:\n",
    "                    indent(1)\n",
    "                    info(f'at iteration {i}')\n",
    "                continue\n",
    "            \n",
    "            l = comatrix.loc[basis].sum() - k\n",
    "            m = comatrix[target].sum() - k\n",
    "            n = comatrix.values.sum() - (k+l+m)\n",
    "            ll = loglikelihood(k, l, m, n, safe_log)\n",
    "            new_matrix[target][basis] = ll\n",
    "\n",
    "            i += 1\n",
    "            if i % 500000 == 0:\n",
    "                indent(1)\n",
    "                info(f'at iteration {i}')\n",
    "    indent(0)\n",
    "    info(f'FINISHED at iteration {i}')\n",
    "    return new_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s beginning calculations...\n",
      "   |     9.72s at iteration 500000\n",
      "   |       18s at iteration 1000000\n",
      "   |       27s at iteration 1500000\n",
      "   |       34s at iteration 2000000\n",
      "    38s FINISHED at iteration 2252096\n"
     ]
    }
   ],
   "source": [
    "test_ll = apply_loglikelihood(cooccurrences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PMI\n",
    "Pointwise Mutual Information Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "def apply_pmi(col):\n",
    "    \n",
    "    '''\n",
    "    Apply PMI to a given column.\n",
    "    '''\n",
    "    \n",
    "    expected = col * cooccurrences.sum(axis=1) / cooccurrences.values.sum()\n",
    "    pmi = np.log(col / expected).fillna(0)\n",
    "    \n",
    "    return pmi\n",
    "\n",
    "test_pmi = cooccurrences.apply(lambda k: apply_pmi(k))\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(vectA, vectB):\n",
    "    '''\n",
    "    Calculate the similarity between a supplied vector A and vector B.\n",
    "    '''\n",
    "    new_matrix = comatrix.copy()\n",
    "    i = 0 \n",
    "    indent(reset=True)\n",
    "    info('beginning calculations...')\n",
    "    indent(1, reset=True)\n",
    "    \n",
    "    cos = sum(vectA * vectB) / (math.sqrt(sum(vectA*vectA)) * math.sqrt(sum(vectB*vectB)))\n",
    "    \n",
    "    for target in comatrix.columns:\n",
    "        for basis in comatrix.index:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar nouns to CMC/:\n",
      "\n",
      "[0.4150043639884484, 'JRX=/', 'moon', 27]\n",
      "[0.3161405227898356, 'QY/', 'end', 67]\n",
      "[0.29466753668347084, 'KWKB/', 'star', 37]\n",
      "[0.24312437451112762, 'QYP/', 'anger', 28]\n",
      "[0.2395651181474568, 'FVN/', 'adversary', 27]\n",
      "[0.2374447820273706, 'NG</', 'stroke', 78]\n",
      "[0.21233345826232736, 'LBN/', 'white', 29]\n",
      "[0.21125236182853413, '>XWR/', 'back(wards)', 41]\n",
      "[0.21124328795843902, 'CMH/', 'destruction', 41]\n",
      "[0.21124328795843902, '<WD/', 'duration', 490]\n",
      "[0.20947448300570387, '>WR/', 'light', 115]\n",
      "[0.19948636735047134, 'GDWD/', 'band', 33]\n",
      "[0.19919327844060208, 'MF>/', 'burden', 44]\n",
      "[0.1921868436569057, 'TWDH/', 'thanksgiving', 32]\n",
      "[0.1903742354218136, 'MWPT/', 'sign', 36]\n",
      "[0.17972525000054967, 'JHWDJ/', 'Jewish', 82]\n",
      "[0.17161640991552882, 'LB/', 'heart', 601]\n",
      "[0.17139924370651088, '<CN/', 'smoke', 25]\n",
      "[0.17106615836304698, 'ML>K/', 'messenger', 213]\n",
      "[0.16834566202038767, '>WT/', 'sign', 79]\n",
      "[0.16004780497575885, 'XRWN/', 'anger', 41]\n",
      "[0.15921924527229037, 'JC</', 'help', 36]\n",
      "[0.15915308411913814, 'QBR/', 'grave', 67]\n",
      "[0.15846823463310694, 'DWR/', 'generation', 166]\n",
      "[0.15656369998522884, 'XRB/', 'dagger', 412]\n"
     ]
    }
   ],
   "source": [
    "test = test_ll\n",
    "\n",
    "testw = 'CMC/' # test sun\n",
    "\n",
    "def show_sim(testw, test):\n",
    "    \n",
    "    test_sims = []\n",
    "    vectA = test[testw]\n",
    "\n",
    "    print(f'Similar nouns to {testw}:\\n')\n",
    "\n",
    "    for word in test.columns:\n",
    "\n",
    "        if word  == testw:\n",
    "            continue\n",
    "\n",
    "        vectB = test[word]\n",
    "\n",
    "        cosine = sum(vectA * vectB) / (math.sqrt(sum(vectA*vectA)) * math.sqrt(sum(vectB*vectB)))\n",
    "\n",
    "        test_sims.append((cosine, word))\n",
    "\n",
    "    for score, word in sorted(test_sims, reverse=True)[:25]:\n",
    "\n",
    "        lex = [lex for lex in F.otype.s('lex') if F.lex.v(lex) == word][0]\n",
    "\n",
    "        print([score, word, F.gloss.v(lex), F.freq_lex.v(lex)])\n",
    "        \n",
    "show_sim(testw, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar nouns to CMC/:\n",
      "\n",
      "[0.4179322089361344, 'JRX=/', 'moon', 27]\n",
      "[0.23525002406994303, 'KWKB/', 'star', 37]\n",
      "[0.17617422385425718, '>WR/', 'light', 115]\n",
      "[0.17206312358043505, 'QY/', 'end', 67]\n",
      "[0.15512414634439806, 'NG</', 'stroke', 78]\n",
      "[0.1483442427192811, 'FVN/', 'adversary', 27]\n",
      "[0.14445147965146882, 'LBN/', 'white', 29]\n",
      "[0.1291546158113168, '>XWR/', 'back(wards)', 41]\n",
      "[0.1290635165594666, 'CMH/', 'destruction', 41]\n",
      "[0.1290635165594666, '<WD/', 'duration', 490]\n",
      "[0.10970210464824995, 'TWDH/', 'thanksgiving', 32]\n",
      "[0.10800930917389484, '<FR/', '-teen', 203]\n",
      "[0.10529720026553621, 'LBB/', 'heart', 252]\n",
      "[0.10187635307751156, 'XRWN/', 'anger', 41]\n",
      "[0.10165501228030492, 'QYP/', 'anger', 28]\n",
      "[0.09886326158042938, 'JWM/', 'day', 2304]\n",
      "[0.0978175375382063, 'LB/', 'heart', 601]\n",
      "[0.09346768755395984, 'CW>/', 'vanity', 53]\n",
      "[0.09270785479141942, 'NXL/', 'wadi', 139]\n",
      "[0.09263273402933128, 'KZB/', 'lie', 31]\n",
      "[0.09095904311726694, 'MWPT/', 'sign', 36]\n",
      "[0.08915226805645277, 'JHWDJ/', 'Jewish', 82]\n",
      "[0.08674710109226119, 'HMWN/', 'commotion', 84]\n",
      "[0.08637979401156666, 'MF>/', 'burden', 44]\n",
      "[0.08602941699114085, 'QRWB/', 'near', 76]\n"
     ]
    }
   ],
   "source": [
    "show_sim(testw, test_pmi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
