{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Selection Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 3.2.2\n",
      "Api reference : https://github.com/Dans-labs/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/Dans-labs/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Example data  : https://github.com/Dans-labs/text-fabric-data\n",
      "\n",
      "115 features found and 0 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.01s B book                 from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.01s B chapter              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.01s B verse                from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.06s B function             from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.11s B sp                   from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.11s B pdp                  from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.18s B mother               from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.18s B rela                 from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.18s B typ                  from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.10s B lex                  from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.10s B ls                   from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s Feature overview: 109 for nodes; 5 for edges; 1 configs; 7 computed\n",
      "  5.16s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "# First, I load the necesssary modules, data, and helper functions.\n",
    "import collections, random\n",
    "from tf.fabric import Fabric\n",
    "from functions.helpers import show_results, filter_results\n",
    "\n",
    "# load BHSA data into TF\n",
    "TF = Fabric(locations=['~/github/etcbc/bhsa/tf', '~/github/semantics/tf'], modules='c')\n",
    "api = TF.load('''\n",
    "                book chapter verse\n",
    "                function sp pdp mother\n",
    "                rela typ lex ls \n",
    "              ''')\n",
    "api.makeAvailableIn(globals()) # globalize TF methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Head Noun Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_phrs_type(phrase_atom, subphrases, diagnose=False):\n",
    "    '''\n",
    "    Return boolean on whether a phrase atom is an acceptable type.\n",
    "    Acceptable is either a noun phrase (NP) or\n",
    "    a prepositional phrase (PP) that is governed only by את.\n",
    "    '''\n",
    "\n",
    "    if F.typ.v(phrase_atom) == 'NP': # noun phrase\n",
    "        return True\n",
    "    \n",
    "    # for logic on this selection criteria, see [?]\n",
    "    prep_sp = sorted(sp for sp in subphrases # sorted sp with prepositions\n",
    "                         if 'prep' in set(F.pdp.v(w) for w in L.d(sp, 'word')))\n",
    "    phrase_type = prep_sp or (phrase_atom,)\n",
    "    preps = [w for w in L.d(phrase_type[0], 'word') \n",
    "                    if F.pdp.v(w) == 'prep']\n",
    "    \n",
    "    if F.typ.v(phrase_atom) == 'PP' and preps: # check for את\n",
    "        prep = preps[0]\n",
    "        if F.lex.v(prep) == '>T':\n",
    "            return True\n",
    "        else:\n",
    "            if diagnose:\n",
    "                print('>T not found...')\n",
    "            return False\n",
    "    else:\n",
    "        if diagnose:\n",
    "            print('neither NP or PP...')\n",
    "            print('phrase_type: ', phrase_type)\n",
    "        return False\n",
    "\n",
    "def get_genitive(abs_wnode, good_pdp, good_sp, diagnose=False):\n",
    "    '''\n",
    "    Extract the head noun in a כֹל construct chain.\n",
    "    The function simply returns the first substantive in the chain.\n",
    "    '''\n",
    "    \n",
    "    rectum = E.mother.t(abs_wnode) # get rectum subphrase\n",
    "    abs_phrase = L.d(L.u(abs_wnode, 'phrase')[0], 'word') # for phrase boundary\n",
    "    \n",
    "    if not rectum:\n",
    "        if diagnose:\n",
    "            print('no rectum found at word', abs_wnode)\n",
    "        return None  # abs not in norm. construct (e.g. w/ verbs)\n",
    "    \n",
    "    # get words and nouns in the rectum subphrase\n",
    "    r_words = L.d(rectum[0], 'word')\n",
    "    r_nouns = [w for w in r_words \n",
    "                   if F.sp.v(w) in good_sp\n",
    "                   and F.pdp.v(w) in good_pdp\n",
    "                   and w in abs_phrase]\n",
    "    if r_nouns:\n",
    "        return r_nouns[0] # return the first noun\n",
    "    else:\n",
    "        if diagnose:\n",
    "            print('no noun found for word', abs_wnode)\n",
    "        return None # no noun found, return nothing\n",
    "    \n",
    "def independent(phrase_atom, subphrases, heads_list):\n",
    "    \n",
    "    '''\n",
    "    Checks phrase and subphrase relations for dependency relations.\n",
    "    Requires a list of previously analyzed head nouns.\n",
    "    This list is required to double check parallel (coordinate) relations. \n",
    "    '''\n",
    "    # exclude words in phrase_atoms with these relation features\n",
    "    omit_pa_rela = {'Appo', # apposition\n",
    "                    'Spec'} # specification\n",
    "    \n",
    "    # exclude words in subphrases with these relation features\n",
    "    omit_sp_rela = {'rec', # nomen rectum\n",
    "                    'adj', # adjunct \n",
    "                    'atr', # attributive\n",
    "                    'mod', # modifier\n",
    "                    'dem'} # demontrative\n",
    "    \n",
    "    parallels = {'par', 'Para'} # parallel i.e. coordination specification\n",
    "    omit_relas = omit_pa_rela | omit_sp_rela\n",
    "    phrase_units = list(subphrases) + [phrase_atom]\n",
    "    \n",
    "    relas = set(F.rela.v(obj) for obj in list(subphrases) + [phrase_atom])\n",
    "    \n",
    "    if not relas & omit_relas and not parallels & relas: # good relas\n",
    "        return True\n",
    "    \n",
    "    elif not relas & omit_relas and parallels & relas: # check parallel relations\n",
    "        \n",
    "        # assemble acceptable phrase mothers from the already accepted head nouns\n",
    "        head_mothers = set(L.u(w, 'phrase_atom')[0] for w in heads_list)\n",
    "        head_mothers |= set(L.u(w, 'subphrase') for w in heads_list)\n",
    "        \n",
    "        for pu in phrase_units:\n",
    "            if F.rela.v(pu) in parallels:\n",
    "                mother = E.mother.f(pu)[0]\n",
    "                if mother in head_mothers:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "                \n",
    "    else: # noun is not independent\n",
    "        return False  \n",
    "        \n",
    "    \n",
    "def get_heads(phrase, diagnose=False):\n",
    "    '''\n",
    "    Returns substantive head nouns, if there are any, from a phrase node.\n",
    "    \"substantive\" does not include prounouns.\n",
    "    \n",
    "    Based on a supplied phrase get phrase atom and subphrase features \n",
    "    and compare them against a group of sets.\n",
    "    Define those sets first. Then make the comparison.\n",
    "    \n",
    "    *Caution* \n",
    "    This function works reasonably well,\n",
    "    but there are a number of edge cases that it does not catch.\n",
    "    Fine-tuning this function would make a nice notebook in itself.\n",
    "    See Gen 20:5 for a good edge case example, in which both היא pronouns\n",
    "    are registered as subjects, but only one should be.\n",
    "    '''\n",
    "    \n",
    "    good_sp = {'subs', 'nmpr', 'adjv'}\n",
    "    good_pdp = {'subs', 'nmpr'}\n",
    "        \n",
    "    heads = [] # nouns go here\n",
    "    phrase_words = L.d(phrase, 'word')\n",
    "        \n",
    "    for word in phrase_words:\n",
    "        \n",
    "        # get phrases's phrase atoms, subphrases, and subphrase relations\n",
    "        phrase_atom = L.u(word, 'phrase_atom')[0]\n",
    "        subphrases = L.u(word, 'subphrase') \n",
    "        sp_relas = set(F.rela.v(sp) for sp in subphrases)\n",
    "        \n",
    "        test_good = [F.pdp.v(word) in good_pdp, # is noun\n",
    "                     F.sp.v(word) in good_sp, # is noun\n",
    "                     good_phrs_type(phrase_atom, subphrases, diagnose), # is NP or PP with את\n",
    "                     independent(phrase_atom, subphrases, heads) \n",
    "                    ] # is valid subphrase rela.\n",
    "        \n",
    "        # compare word/phrase features\n",
    "        if all(test_good):\n",
    "        \n",
    "            # handle quantifiers\n",
    "            if F.lex.v(word) == 'KL/' or F.ls.v(word) == 'card':\n",
    "                genitive_head = get_genitive(word, good_pdp, good_sp, diagnose) # returns word node or None\n",
    "                if genitive_head:\n",
    "                    heads.append(genitive_head) # valid quantified noun found\n",
    "                else:\n",
    "                    continue # no noun found, skip it\n",
    "            else:\n",
    "                heads.append(word) # word is a head\n",
    "    \n",
    "        else:\n",
    "            if diagnose: \n",
    "                print(T.text([word]), word)\n",
    "                print('test_good', tuple(zip(test_good, ('pdp', 'sp', 'phr_typ', 'pAt_rela', 'sp_rela'))))\n",
    "                print('subphrases', subphrases)\n",
    "                print('phrase_atom', phrase_atom)\n",
    "                print()\n",
    "            continue\n",
    "            \n",
    "    return heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject and Object Omissions\n",
    "\n",
    "A previous version of the valid preposition function only identified nouns from prepositional phrase atoms that spanned the entire functional phrase. That omits cases such as Josh 24:18 with constructs:\n",
    "> ('Joshua', 24, 18) <br>\n",
    "> 722643 אֶת־כָּל־הָעַמִּ֗ים וְאֶת־הָאֱמֹרִ֛י <br>\n",
    "\n",
    "The search below identifies phrases that begin with a preposition besides את and function as an object or subject (N.B. the prep. את can mark subjects in passive constructions). These are the cases which will be excluded by the new version of the function. A survey of these cases confirms that none of them contain nouns that are of interest: that is, none of these prepositions appear to grammaticaly mark a subject or object, but appear to be specifiers. \n",
    "\n",
    "For the old function, see `is_preposition_subj` in the [old version](https://github.com/codykingham/tfNotebooks/blob/master/4Q246_Participants/participant_functions/subjects.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 results\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 18px; line-height: 1\">1. Genesis 40:17</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 24px; line-height: 1\">וּבַסַּ֣ל הָֽעֶלְיֹו֔ן <span style=\"color: blue\">מִ</span><span style=\"color: blue\">כֹּ֛ל </span><span style=\"color: blue\">מַאֲכַ֥ל </span><span style=\"color: blue\">פַּרְעֹ֖ה </span><span style=\"color: blue\">מַעֲשֵׂ֣ה </span><span style=\"color: blue\">אֹפֶ֑ה </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 18px; line-height: 1\">2. Genesis 44:18</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 24px; line-height: 1\">כִּ֥י <span style=\"color: green\">כָמֹ֖וךָ </span>כְּפַרְעֹֽה׃ </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 18px; line-height: 1\">3. Genesis 47:26</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 24px; line-height: 1\">לְפַרְעֹ֖ה <span style=\"color: blue\">לַ</span><span style=\"color: blue\"></span><span style=\"color: blue\">חֹ֑מֶשׁ </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 18px; line-height: 1\">4. Exodus 4:9</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 24px; line-height: 1\">וְלָקַחְתָּ֙ <span style=\"color: green\">מִ</span><span style=\"color: green\">מֵּימֵ֣י </span><span style=\"color: green\">הַ</span><span style=\"color: green\">יְאֹ֔ר </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 18px; line-height: 1\">5. Exodus 17:5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 24px; line-height: 1\">וְקַ֥ח אִתְּךָ֖ <span style=\"color: blue\">מִ</span><span style=\"color: blue\">זִּקְנֵ֣י </span><span style=\"color: blue\">יִשְׂרָאֵ֑ל </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- \n",
      "\n",
      "results cut off at 5\n"
     ]
    }
   ],
   "source": [
    "targets = []\n",
    "\n",
    "for phrase in F.otype.s('phrase'):\n",
    "    \n",
    "    if F.function.v(phrase) not in {'Subj', 'Objc'}:\n",
    "        continue\n",
    "    \n",
    "    phrase_atoms = L.d(phrase, 'phrase_atom')\n",
    "    pa_lex = set(F.lex.v(w) for w in L.d(phrase_atoms[0], 'word'))\n",
    "        \n",
    "    if F.typ.v(phrase_atoms[0]) == 'PP' and '>T' not in pa_lex:\n",
    "        \n",
    "        targets.append((L.u(phrase, 'clause')[0], phrase, phrase_atoms[0]))\n",
    "        \n",
    "show_results(targets, limit=5, highlight=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further inspection of subject or object phrases that do not begin with את but contain את later on in the phrase shows that most of these are cases of adjectival specification, with a few parallel relationships reflected. In particular, it was important to be sure that את in the adjectival sense, especially where it has the sense of \"with\" rather than a grammatical one, would be excluded from the noun selector. Many of the finds by this search were more appositional than used in this \"with sense\". But a handful were found. 1 Chronicles 20:5, broken down below this search, confirms that את phrases in this adjectival sense are marked as `Spec` for specification. Thus, it is completely safe in the preposition parser function to take any את prepositional phrase. The acceptable subphrase relation set will then eliminate any match that is a specifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 results\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 18px; line-height: 1\">1. Genesis 6:10</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 24px; line-height: 1\">וַיֹּ֥ולֶד נֹ֖חַ <span style=\"color: blue\">שְׁלֹשָׁ֣ה </span><span style=\"color: blue\">בָנִ֑ים </span><span style=\"color: green\">אֶת־</span><span style=\"color: green\">שֵׁ֖ם </span><span style=\"color: green\">אֶת־</span><span style=\"color: green\">חָ֥ם </span><span style=\"color: green\">וְ</span><span style=\"color: green\">אֶת־</span><span style=\"color: green\">יָֽפֶת׃ </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 18px; line-height: 1\">2. Genesis 40:8</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 24px; line-height: 1\">וּ<span style=\"color: blue\">פֹתֵ֖ר </span>אֵ֣ין <span style=\"color: green\">אֹתֹ֑ו </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 18px; line-height: 1\">3. Genesis 41:15</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 24px; line-height: 1\">וּ<span style=\"color: blue\">פֹתֵ֖ר </span>אֵ֣ין <span style=\"color: green\">אֹתֹ֑ו </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 18px; line-height: 1\">4. Exodus 1:11</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 24px; line-height: 1\">וַיִּ֜בֶן <span style=\"color: blue\">עָרֵ֤י </span><span style=\"color: blue\">מִסְכְּנֹות֙ </span>לְפַרְעֹ֔ה <span style=\"color: green\">אֶת־</span><span style=\"color: green\">פִּתֹ֖ם </span><span style=\"color: green\">וְ</span><span style=\"color: green\">אֶת־</span><span style=\"color: green\">רַעַמְסֵֽס׃ </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 18px; line-height: 1\">5. Exodus 35:25</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 24px; line-height: 1\">וַיָּבִ֣יאוּ <span style=\"color: blue\">מַטְוֶ֗ה </span><span style=\"color: green\">אֶֽת־</span><span style=\"color: green\">הַ</span><span style=\"color: green\">תְּכֵ֨לֶת֙ </span><span style=\"color: green\">וְ</span><span style=\"color: green\">אֶת־</span><span style=\"color: green\">הָֽ</span><span style=\"color: green\">אַרְגָּמָ֔ן </span><span style=\"color: green\">אֶת־</span><span style=\"color: green\">תֹּולַ֥עַת </span><span style=\"color: green\">הַ</span><span style=\"color: green\">שָּׁנִ֖י </span><span style=\"color: green\">וְ</span><span style=\"color: green\">אֶת־</span><span style=\"color: green\">הַ</span><span style=\"color: green\">שֵּֽׁשׁ׃ </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- \n",
      "\n",
      "results cut off at 5\n"
     ]
    }
   ],
   "source": [
    "targets = []\n",
    "\n",
    "for phrase in F.otype.s('phrase'):\n",
    "    \n",
    "    if F.function.v(phrase) not in {'Subj', 'Objc'}:\n",
    "        continue\n",
    "    \n",
    "    phrase_atoms = L.d(phrase, 'phrase_atom')\n",
    "    first_pa = phrase_atoms[0]\n",
    "    f_pa_lex = set(F.lex.v(w) for w in L.d(first_pa, 'word'))\n",
    "    \n",
    "    if '>T' in f_pa_lex:\n",
    "        continue\n",
    "    \n",
    "    for i in range(1, len(phrase_atoms)):\n",
    "        \n",
    "        other_pa = phrase_atoms[i]\n",
    "        pa_lex = set(F.lex.v(w) for w in L.d(other_pa, 'word'))\n",
    "        \n",
    "        if F.typ.v(other_pa) == 'PP' and '>T' in pa_lex:\n",
    "\n",
    "            targets.append((L.u(phrase, 'clause')[0], phrase, other_pa))\n",
    "            break\n",
    "        \n",
    "show_results(targets, limit=5, highlight=[1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen below, \"war *with Philistines*\" is marked as `Spec`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1158377\n",
      "מִלְחָמָ֖ה \n",
      "NA\n",
      "\n",
      "1158378\n",
      "אֶת־פְּלִשְׁתִּ֑ים \n",
      "Spec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (\"1_Chronicles\", 20, 5)\n",
    "for sp in (L.d(892236, 'phrase_atom')):\n",
    "    print(sp)\n",
    "    print(T.text(L.d(sp, 'word')))\n",
    "    print(F.rela.v(sp))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordering of Subphrases on a L.u call from Word Node\n",
    "\n",
    "This search demonstrates that subphrase nodes are indeed ordered by size when called from a word node, i.e. subphrases that contain less words receive smaller node numbers. If they are equal in size, either one might have the lower node number.\n",
    "\n",
    "It is indeed acceptable to select the first subphrase on a `L.u` call from a word for preposition testing. That is the subphrase which will be closest to the word level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_true = []\n",
    "true = 0\n",
    "no_sp = 0\n",
    "\n",
    "for word in F.otype.s('word'):\n",
    "    \n",
    "    subphrases = sorted(L.u(word, 'subphrase'))\n",
    "    \n",
    "    if not subphrases:\n",
    "        no_sp += 1\n",
    "        continue\n",
    "        \n",
    "    sp_len = sorted((len(L.d(sp, 'word')), sp) for sp in subphrases) # sort by word length\n",
    "    sp_check = [sp[1] for sp in sp_len] # iterate over sorted list and grab subphrase nodes\n",
    "    \n",
    "    if sp_check == subphrases: # check them\n",
    "        true += 1\n",
    "        \n",
    "    else:\n",
    "        not_true.append(word)\n",
    "\n",
    "len(not_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test the function\n",
    "\n",
    "# results = []\n",
    "# ct = 0\n",
    "\n",
    "# object_phrases = list(F.function.s('Objc'))\n",
    "\n",
    "# random.shuffle(object_phrases)\n",
    "\n",
    "# for phrase in object_phrases:\n",
    "    \n",
    "#     heads = get_heads(phrase)\n",
    "    \n",
    "#     if heads and len(heads) < len(L.d(phrase, 'word')):\n",
    "        \n",
    "#         print(T.sectionFromNode(phrase))\n",
    "#         print(T.text(L.d(L.u(phrase, 'clause')[0], 'word')))\n",
    "#         print(phrase)\n",
    "#         print(T.text(L.d(phrase, 'word')))\n",
    "#         heads = [T.text([head]) for head in heads]\n",
    "#         print(' | '.join(heads))\n",
    "#         print()\n",
    "        \n",
    "#         ct += 1\n",
    "        \n",
    "#         if ct == 100:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Many Phrases Does it Validate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'heads_found': 36417,\n",
       "         'pronoun_excluded': 5740,\n",
       "         'total': 54599,\n",
       "         'unknown': 12441})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_counts = collections.Counter()\n",
    "\n",
    "for phrase in F.otype.s('phrase'):\n",
    "    \n",
    "    if F.function.v(phrase) not in {'Subj', 'Objc'}:\n",
    "        continue\n",
    "        \n",
    "    heads = get_heads(phrase)\n",
    "    pdps = set(F.pdp.v(w) for w in L.d(phrase))\n",
    "    \n",
    "    phrase_counts['total'] += 1\n",
    "    \n",
    "    if heads:\n",
    "        phrase_counts['heads_found'] += 1\n",
    "        \n",
    "    elif pdps & {'prps', 'prde', 'prin'}:\n",
    "        phrase_counts['pronoun_excluded'] += 1\n",
    "    \n",
    "    elif pdps & {'intj'}:\n",
    "        phrase_counts['interjection_excluded']\n",
    "    \n",
    "    else:\n",
    "        phrase_counts['unknown'] += 1     \n",
    "        \n",
    "phrase_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push to TF as Edges\n",
    "\n",
    "Apply the function phrase atoms and phrases that serve as subjects or objects in the BHSA. The relationships will be pushed out to TF as an edge relation from a phrase to its head noun word nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 3.2.2\n",
      "Api reference : https://github.com/Dans-labs/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/Dans-labs/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Example data  : https://github.com/Dans-labs/text-fabric-data\n",
      "\n",
      "0 features found and 0 ignored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s Warp feature \"otype\" not found in\n",
      "\n",
      "  0.00s Warp feature \"oslots\" not found in\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Warp feature \"otext\" not found. Working without Text-API\n",
      "\n",
      "  0.00s Exporting 0 node and 1 edge and 0 config features to tf/c:\n",
      "   |     0.20s T heads                to tf/c\n",
      "  0.20s Exported 0 node features and 1 edge features and 0 config features to tf/c\n"
     ]
    }
   ],
   "source": [
    "meta = {'': {'created_by': 'Cody Kingham',\n",
    "             'coreData': 'BHSA',\n",
    "             'coreVersion': 'c'\n",
    "            },\n",
    "        'heads' : {'source': 'see the notebooks at https://github.com/codykingham/semantics',\n",
    "                  'valueType': 'int',\n",
    "                  'edgeValues': False}\n",
    "       }\n",
    "\n",
    "heads = {}\n",
    "\n",
    "for phrase in F.otype.s('phrase'):\n",
    "    \n",
    "    # only push features for these two types for now\n",
    "    if F.function.v(phrase) not in {'Subj', 'Objc'}:\n",
    "        continue\n",
    "    \n",
    "    phrase_heads = get_heads(phrase)\n",
    "    \n",
    "    if phrase_heads:\n",
    "        heads[phrase] = set(phrase_heads)\n",
    "    \n",
    "    for phrase_atom in L.d(phrase, 'phrase_atom'):\n",
    "        phraseAt_heads = get_heads(phrase_atom)\n",
    "        if phraseAt_heads:\n",
    "            heads[phrase_atom] = set(phraseAt_heads)\n",
    "        \n",
    "new_edges = {'heads': heads}\n",
    "\n",
    "saveTF = Fabric('tf/c')\n",
    "saveTF.save(nodeFeatures={}, edgeFeatures=new_edges, metaData=meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Heads Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s loading features ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   |     0.00s Feature \"heads\" not available in\n",
      "   |   /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |   \t/Users/cody/github/semantics/tf/c\n",
      "  0.01s Not all features could be loaded/computed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |     0.00s M otext                from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.03s B otype                from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.51s B oslots               from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M otext                from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.01s B book                 from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.01s B chapter              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.01s B verse                from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.12s B g_cons               from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.16s B g_cons_utf8          from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.12s B g_lex                from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.18s B g_lex_utf8           from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.14s B g_word               from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.18s B g_word_utf8          from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.10s B lex0                 from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.14s B lex_utf8             from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B qere                 from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B qere_trailer         from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B qere_trailer_utf8    from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B qere_utf8            from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.06s B trailer              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.07s B trailer_utf8         from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B __levels__           from otype, oslots, otext\n",
      "   |     0.03s B __order__            from otype, oslots, __levels__\n",
      "   |     0.04s B __rank__             from otype, __order__\n",
      "   |     0.95s B __levUp__            from otype, oslots, __rank__\n",
      "   |     0.71s B __levDown__          from otype, __levUp__, __rank__\n",
      "   |     0.29s B __boundary__         from otype, oslots, __rank__\n",
      "   |     0.01s B __sections__         from otype, oslots, otext, __levUp__, __levels__, book, chapter, verse\n",
      "   |     0.06s B function             from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.11s B sp                   from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.11s B pdp                  from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.18s B mother               from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.18s B rela                 from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.18s B typ                  from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.10s B lex                  from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.10s B ls                   from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B book@am              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B book@ar              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B book@bn              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B book@da              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B book@de              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B book@el              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B book@en              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B book@es              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B book@fa              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B book@fr              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B book@he              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B book@hi              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B book@id              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B book@ja              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B book@ko              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B book@la              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B book@nl              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B book@pa              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B book@pt              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B book@ru              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B book@sw              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B book@syc             from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B book@tr              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B book@ur              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B book@yo              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B book@zh              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M code                 from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M det                  from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M dist                 from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M dist_unit            from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M distributional_parent from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M domain               from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M freq_lex             from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M freq_occ             from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M functional_parent    from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M g_nme                from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M g_nme_utf8           from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M g_pfm                from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M g_pfm_utf8           from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M g_prs                from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M g_prs_utf8           from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M g_uvf                from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M g_uvf_utf8           from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M g_vbe                from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M g_vbe_utf8           from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M g_vbs                from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M g_vbs_utf8           from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M gloss                from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M gn                   from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M instruction          from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M is_root              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M kind                 from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M kq_hybrid            from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M kq_hybrid_utf8       from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M label                from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M language             from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M languageISO          from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M lexeme_count         from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M mother_object_type   from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M nametype             from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M nme                  from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M nu                   from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M number               from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M omap@2017-c          from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M otext                from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M pargr                from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M pfm                  from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M prs                  from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M prs_gn               from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M prs_nu               from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M prs_ps               from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M ps                   from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M rank_lex             from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M rank_occ             from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M root                 from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M st                   from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M suffix_gender        from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M suffix_number        from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M suffix_person        from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M tab                  from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M txt                  from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M uvf                  from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M vbe                  from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M vbs                  from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M voc_lex              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M voc_lex_utf8         from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M vs                   from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M vt                   from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s M otext                from /Users/cody/github/etcbc/bhsa/tf/c\n"
     ]
    }
   ],
   "source": [
    "TF.load('heads', add=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EdgeFeatures' object has no attribute 'heads'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d775713d63b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mclause\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clause'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EdgeFeatures' object has no attribute 'heads'"
     ]
    }
   ],
   "source": [
    "ct = 0 \n",
    "\n",
    "for phrase in F.otype.s('phrase'):\n",
    "    \n",
    "    if F.function.v(phrase) not in {'Subj', 'Objc'}:\n",
    "        continue\n",
    "        \n",
    "    if E.heads.f(phrase):\n",
    "        \n",
    "        clause = L.u(phrase, 'clause')[0]\n",
    "        \n",
    "        print(T.sectionFromNode(phrase))\n",
    "        print(T.text(L.d(clause, 'word')))\n",
    "        print(T.text(L.d(phrase, 'word')))\n",
    "        print(E.heads.f(phrase))\n",
    "        print(T.text(E.heads.f(phrase)))\n",
    "        print()\n",
    "    \n",
    "    if ct > 100:\n",
    "        break\n",
    "        \n",
    "    ct += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
