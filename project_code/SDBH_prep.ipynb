{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration and Prep of SDBH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections, Levenshtein, re\n",
    "import xml.etree.ElementTree as ET\n",
    "from tf.fabric import Fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 4.1.2\n",
      "Api reference : https://dans-labs.github.io/text-fabric/Api/General/\n",
      "Tutorial      : https://github.com/Dans-labs/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Example data  : https://github.com/Dans-labs/text-fabric-data\n",
      "\n",
      "114 features found and 0 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.01s B book                 from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B chapter              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B verse                from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.16s B lex_utf8             from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B qere                 from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.11s B lex                  from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B voc_lex_utf8         from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.11s B pdp                  from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s Feature overview: 109 for nodes; 4 for edges; 1 configs; 7 computed\n",
      "  4.43s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "TF = Fabric(locations='~/github/etcbc/bhsa/tf/c')\n",
    "api = TF.load('''\n",
    "              book chapter verse\n",
    "              lex qere \n",
    "              voc_lex_utf8\n",
    "              lex_utf8 pdp\n",
    "              ''')\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdbh_resource = '/Users/cody/github/marble-lexicon/SDBH/SDBH.XML'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdbh_tree = ET.parse(sdbh_resource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = sdbh_tree.getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion To TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map SDBH Domains to Domain Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = '/Users/cody/github/marble-lexicon/SDBH/SDBH.DM1'\n",
    "domains2 = '/Users/cody/github/marble-lexicon/SDBH/SDBH.DM2'\n",
    "\n",
    "domain2code = {}\n",
    "\n",
    "with open(domains, 'r') as infile:\n",
    "    domains = [dm.split('\\\\') + ['version 1'] for dm in infile.read().split('\\n\\n') \n",
    "                   if ''.join(dm.split('\\\\'))] # <- avoid null lines\n",
    "\n",
    "with open(domains2, 'r') as infile2:\n",
    "    domains.extend([dm.split('\\\\') + ['version 2'] for dm in infile2.read().split('\\n\\n') \n",
    "                       if ''.join(dm.split('\\\\'))] \n",
    "                  )\n",
    "    \n",
    "for i, dom in enumerate(domains):\n",
    "    dom_data = dict((data.split(' ', 1)[0], data.split(' ', 1)[1]) for data in dom\n",
    "                        if data.split())\n",
    "    \n",
    "    if 'label' in dom_data and 'code' in dom_data:          \n",
    "        domain2code[dom_data['label'].strip()] = dom_data['version'] + '.' + dom_data['code'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Lexical Domains to Verse and Word References\n",
    "\n",
    "### Note of Caution\n",
    "\n",
    "In this section I find a number of domains without matches in the SDBH.DM1 and SDBH.DM2 files. However, these codes are often formed with categories from those domain files. Some examples are:\n",
    "\n",
    "> 'Kinship > Officials'<br>\n",
    "> 'Parts: Plants'\n",
    "\n",
    "Another one that I have found, however, does not seem to have any direct correspondence:\n",
    "\n",
    "> חוּץ - 'Referents of Location'\n",
    "\n",
    "I am not sure why this item does not have a corresponding code.\n",
    "\n",
    "For this items, I have chosen for now to split the codes into their individual parts and match them to the corresponding codes from the two domain files. For domains like \"Kinship > Officials,\" this appears to work quite well. For some others, though, such as חוץ, caution should be exercised.\n",
    "\n",
    "I am not convinced that this is the best method going forward, but for my project it should work fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2770 cautions registered\n"
     ]
    }
   ],
   "source": [
    "ref2domains = collections.defaultdict(dict)\n",
    "cautions = []\n",
    "\n",
    "for entry in root.findall('Lexicon_Entry'):\n",
    "    \n",
    "    this_lex = entry.attrib['Lemma']\n",
    "    \n",
    "    for meaning in entry.findall('BaseForms/BaseForm/LEXMeanings/'):\n",
    "        \n",
    "        domains = [mean.text for mean in meaning.findall('LEXDomains/LEXDomain')]\n",
    "        domains = [word for word in domains \n",
    "                      if word in domain2code]\n",
    "        \n",
    "        domains = '|'.join(domains)\n",
    "        \n",
    "        if not domains: # try again\n",
    "            domains = [mean.text for mean in meaning.findall('LEXDomains/LEXDomain')]\n",
    "            \n",
    "            if domains: # track unmatched domains\n",
    "                cautions.append(domains)\n",
    "            \n",
    "            domains = [word for domstring in domains \n",
    "                          for word in re.findall('|'.join(domain2code.keys()), domstring)]\n",
    "            domains = '|'.join(domains)\n",
    "            \n",
    "            if not domains: # give up\n",
    "                continue\n",
    "            \n",
    "        for ref in meaning.findall('LEXReferences/LEXReference'):\n",
    "\n",
    "            ref2domains[ref.text[:14]][this_lex] = domains\n",
    "            \n",
    "print(len(cautions), 'cautions registered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'אִישׁ': 'People'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref2domains['00100202300042'] # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.001001002003'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain2code['People']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Kinship > Officials'],\n",
       " ['Kinship > Exist'],\n",
       " ['Kinship > Happen'],\n",
       " ['Kinship > Safe'],\n",
       " ['Kinship > Tombs'],\n",
       " ['Groups > Possess'],\n",
       " ['Non-Space > Act'],\n",
       " ['Non-Space > Think'],\n",
       " ['Parts: Plants'],\n",
       " ['Strong > Names of Deities']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cautions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Conversion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = '''Genesis\n",
    "Exodus\n",
    "Leviticus\n",
    "Numbers\n",
    "Deuteronomy\n",
    "Joshua\n",
    "Judges\n",
    "Ruth\n",
    "1_Samuel\n",
    "2_Samuel\n",
    "1_Kings\n",
    "2_Kings\n",
    "1_Chronicles\n",
    "2_Chronicles\n",
    "Ezra\n",
    "Nehemiah\n",
    "Esther\n",
    "Job\n",
    "Psalms\n",
    "Proverbs\n",
    "Ecclesiastes\n",
    "Song_of_songs\n",
    "Isaiah\n",
    "Jeremiah\n",
    "Lamentations\n",
    "Ezekiel\n",
    "Daniel\n",
    "Hosea\n",
    "Joel\n",
    "Amos\n",
    "Obadiah\n",
    "Jonah\n",
    "Micah\n",
    "Nahum\n",
    "Habakkuk\n",
    "Zephaniah\n",
    "Haggai\n",
    "Zechariah\n",
    "Malachi'''.split('\\n')\n",
    "\n",
    "books = dict((i+1, book) for i, book in enumerate(books))\n",
    "consonants = set(letter for w in F.otype.s('word')\n",
    "                 for letter in F.lex_utf8.v(w))\n",
    "consonants = list(consonants)\n",
    "consonants.remove('ׁ')\n",
    "consonants.remove('ׂ')\n",
    "#consonants.append()\n",
    "\n",
    "finals = {'ם': 'מ',\n",
    "          'ן' : 'נ',\n",
    "          'ך' : 'כ',\n",
    "          'ף' : 'פ',\n",
    "          'ץ' : 'צ'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip(word_string):\n",
    "    '''\n",
    "    strips all accentuations\n",
    "    '''\n",
    "    word_string = word_string.replace('־', ' ')\n",
    "    for final in finals:\n",
    "        word_string = word_string.replace(final, finals[final])\n",
    "    return ''.join(w for w in word_string if w in consonants)\n",
    "\n",
    "def with_qere_words(verse, option=1):\n",
    "    \n",
    "    '''\n",
    "    Returns a list of word nodes\n",
    "    where words are repeated in the\n",
    "    case of a qere reading.\n",
    "    '''\n",
    "    \n",
    "    words = L.d(verse, 'word')\n",
    "    qeres = [w for w in words if F.qere.v(w)]\n",
    "    \n",
    "    qeres_count = [(qeres[i+1] - w if i+1 < len(qeres) else 0) for i, w in enumerate(qeres)\n",
    "                      ]\n",
    "    \n",
    "    if option == 1:\n",
    "        for qe, ct in zip(qeres, qeres_count):\n",
    "            if ct != 1:\n",
    "                index = words.index(qe) + 1\n",
    "                words.insert(index, qe)\n",
    "                \n",
    "    elif option == 2:\n",
    "        for qe in qeres:\n",
    "            index = words.index(qe) + 1\n",
    "            words.insert(index, qe)\n",
    "                    \n",
    "    return words\n",
    "    \n",
    "def not_qere(wordnode):\n",
    "    '''\n",
    "    Check's whether a wordnode's\n",
    "    enclosing verse has a qere reading\n",
    "    or not.\n",
    "    '''\n",
    "    \n",
    "    verse = L.u(wordnode, 'verse')[0]\n",
    "    qeres = [w for w in L.d(verse, 'word') if F.qere.v(w)]\n",
    "    \n",
    "    if not qeres:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def look_around(word_node, target_lex, window=4):\n",
    "    '''\n",
    "    A last ditch option for lex matching.\n",
    "    Looks ahead and behind n words.\n",
    "    '''\n",
    "    \n",
    "    verse_words = L.d(L.u(word_node, 'verse')[0], 'word')\n",
    "    nodes = [word_node+i for i in range(-window, window)\n",
    "                if word_node+i in verse_words\n",
    "                and Levenshtein.ratio(strip(F.voc_lex_utf8.v(L.u(word_node+i, 'lex')[0])), target_lex) > 0.7]\n",
    "    \n",
    "    if nodes:\n",
    "        return nodes[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def get_node(ref_string, qere_option=1):\n",
    "    \n",
    "    '''\n",
    "    Uses an SDBH reference ID to\n",
    "    find the corresponding Text-Fabric \n",
    "    word node.\n",
    "    '''\n",
    "\n",
    "    book = books[round(int(ref_string[:3]))]\n",
    "    chapt = round(int(ref_string[3:6]))\n",
    "    verse = round(int(ref_string[6:9]))\n",
    "    word = int(round(int(ref_string[-3:])) / 2) - 1\n",
    "    verse_node = T.nodeFromSection((book, chapt, verse))\n",
    "    verse_words = with_qere_words(verse_node, option=qere_option)\n",
    "    word_node = verse_words[word]\n",
    "    \n",
    "#     if ref_string == '00902000200052':\n",
    "#         print(f'looking at pos {word}')\n",
    "#         print(T.text([word_node]))\n",
    "#         for i, w in enumerate(verse_words):\n",
    "#             print(i, w, T.text([w]))\n",
    "\n",
    "    return word_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Domains to TF Word Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceptions: 112\n",
      "good matches 268271\n"
     ]
    }
   ],
   "source": [
    "word2domain = {}\n",
    "exceptions = []\n",
    "\n",
    "for ref, data in ref2domains.items():\n",
    "    \n",
    "    for lex, domains in data.items():    \n",
    "            \n",
    "        if not domains:\n",
    "            continue\n",
    "            \n",
    "        lex = strip(lex)\n",
    "            \n",
    "        try:\n",
    "            wordnode = get_node(ref)\n",
    "            etcbc_lex = strip(F.lex_utf8.v(wordnode))\n",
    "\n",
    "            if Levenshtein.ratio(etcbc_lex, lex) > 0.7 or etcbc_lex in lex or lex in etcbc_lex:\n",
    "                word2domain[wordnode] = domains\n",
    "                continue\n",
    "\n",
    "            # try a second time with alternative qere disambig\n",
    "            wordnode = get_node(ref, qere_option=2)\n",
    "            etcbc_lex = strip(F.lex_utf8.v(wordnode))\n",
    "            if Levenshtein.ratio(etcbc_lex, lex) > 0.7 or etcbc_lex in lex or lex in etcbc_lex:\n",
    "                word2domain[wordnode] = domains\n",
    "                \n",
    "            elif look_around(wordnode, lex): # third try with a window search\n",
    "                word2domain[look_around(wordnode, lex)] = domains\n",
    "                \n",
    "            elif not_quere(wordnode): # on fourth attempt, if no qere in verse, take the node\n",
    "                word2domain[wordnode] = domains\n",
    "                \n",
    "            else:\n",
    "                exceptions.append((f'{ref}: unmatched lex: SBDH {lex} ≠ ETCBC {etcbc_lex}'))\n",
    "\n",
    "        except:\n",
    "            \n",
    "            try:\n",
    "                wordnode = get_node(ref, qere_option=2)\n",
    "                etcbc_lex = strip(F.lex_utf8.v(wordnode))\n",
    "                if Levenshtein.ratio(etcbc_lex, lex) > 0.7 or etcbc_lex in lex or lex in etcbc_lex:\n",
    "                    word2domain[wordnode] = domains\n",
    "                \n",
    "            except Exception as e:\n",
    "                exceptions.append((f'{ref}: {e}; SBDH lex {lex}'))\n",
    "    \n",
    "print('exceptions:', len(exceptions))\n",
    "print('good matches', len(word2domain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00600400100042: list index out of range; SBDH lex אמר',\n",
       " '01101501800098: list index out of range; SBDH lex אמר',\n",
       " '01500400700054: list index out of range; SBDH lex ארמית',\n",
       " '02600600300076: list index out of range; SBDH lex במה',\n",
       " '01300601100020: list index out of range; SBDH lex בנ',\n",
       " '01300601100022: list index out of range; SBDH lex בנ',\n",
       " '01401101800036: list index out of range; SBDH lex בנ',\n",
       " '00900500600046: list index out of range; SBDH lex גבול',\n",
       " '00902401900044: list index out of range; SBDH lex הרג',\n",
       " '01300601301040: list index out of range; SBDH lex ושני']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exceptions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Troubleshooting Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = '00100900500040'\n",
    "pbook = books[round(int(problem[:3]))]\n",
    "pchapt = round(int(problem[3:6]))\n",
    "pverse = round(int(problem[6:9]))\n",
    "pword = int(round(int(problem[-3:])) / 2) - 1\n",
    "\n",
    "test = T.nodeFromSection((pbook, pchapt, pverse))\n",
    "\n",
    "test_words = with_qere_words(test, option=2)\n",
    "\n",
    "# print(f'problem at {pbook} {pchapt}:{pverse}, {test}')\n",
    "# print(f'seeking word at pos {pword}\\n')\n",
    "\n",
    "# for i, w in enumerate(test_words):\n",
    "#     lex = L.u(w, 'lex')[0]\n",
    "#     print(i, w, F.lex_utf8.v(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map domains to codes to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268271"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2code = {}\n",
    "\n",
    "for w, domains in word2domain.items():\n",
    "    \n",
    "    if '>' in domains:\n",
    "        print(domains)\n",
    "        break\n",
    "    \n",
    "    domains = [word for word in domains.split('|')]\n",
    "\n",
    "    codes = '|'.join(domain2code.get(dom, '') for dom in domains)\n",
    "    \n",
    "    if codes:\n",
    "        word2code[w] = codes\n",
    "        \n",
    "len(word2code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2domain[1136]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Good Matches to TF Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s Feature \"otype\" not available in\n",
      "/Users/cody/github/semantics/project_code/sdbh/\n",
      "  0.00s Not all features could be loaded/computed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |     0.50s T sem_domain           to /Users/cody/github/semantics/project_code/sdbh\n",
      "   |     0.41s T sem_domain_code      to /Users/cody/github/semantics/project_code/sdbh\n",
      "EXPORT DONE!\n"
     ]
    }
   ],
   "source": [
    "meta = {'': {'created_by': 'Renier de Blois (UBS)',\n",
    "         'coreData': 'BHSA',\n",
    "         'coreVersion': 'c'\n",
    "        },\n",
    "        \n",
    "    'sem_domain_code' : {'source': 'Exported from the SDBH.XML',\n",
    "                    'valueType': 'str'},\n",
    "        \n",
    "    'sem_domain': {'source': 'Exported from the SDBH.XML',\n",
    "              'valueType': 'str'}\n",
    "   }\n",
    "\n",
    "newFeatures = {'sem_domain_code': word2code,\n",
    "               'sem_domain': word2domain\n",
    "              }\n",
    "\n",
    "save_TF = Fabric(locations='~/github/semantics/project_code/sdbh', silent=True)\n",
    "api = save_TF.load('', silent=True)\n",
    "\n",
    "save_TF.save(nodeFeatures=newFeatures, edgeFeatures={}, metaData=meta)\n",
    "print('EXPORT DONE!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 4.1.2\n",
      "Api reference : https://dans-labs.github.io/text-fabric/Api/General/\n",
      "Tutorial      : https://github.com/Dans-labs/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Example data  : https://github.com/Dans-labs/text-fabric-data\n",
      "\n",
      "116 features found and 0 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.01s B book                 from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.01s B chapter              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B verse                from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.20s B lex_utf8             from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B qere                 from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.14s B lex                  from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.01s B voc_lex_utf8         from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.14s B pdp                  from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.82s T sem_domain           from /Users/cody/github/semantics/project_code/sdbh\n",
      "   |     0.89s T sem_domain_code      from /Users/cody/github/semantics/project_code/sdbh\n",
      "   |     0.01s B gloss                from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s Feature overview: 111 for nodes; 4 for edges; 1 configs; 7 computed\n",
      "  6.58s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "TF = Fabric(locations=['~/github/etcbc/bhsa/tf/c', '~/github/semantics/project_code/sdbh'])\n",
    "api = TF.load('''\n",
    "              book chapter verse\n",
    "              lex qere \n",
    "              voc_lex_utf8\n",
    "              lex_utf8 pdp\n",
    "              sem_domain \n",
    "              sem_domain_code \n",
    "              gloss\n",
    "              ''')\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "בָּרָ֣א  Exist\n",
      "אֱלֹהִ֑ים  Deities\n",
      "אֵ֥ת  Markers|Identifiers\n",
      "שָּׁמַ֖יִם  Universe\n",
      "אֵ֥ת  Markers|Identifiers\n",
      "אָֽרֶץ׃  Land\n",
      "אָ֗רֶץ  Land\n",
      "הָיְתָ֥ה  Exist\n",
      "תֹ֨הוּ֙  Non-Exist\n",
      "בֹ֔הוּ  Non-Exist\n",
      "חֹ֖שֶׁךְ  Dark\n",
      "עַל־ Location\n",
      "תְהֹ֑ום  Waterbodies\n",
      "ר֣וּחַ  Spirit|Parts|Deities\n",
      "אֱלֹהִ֔ים  Deities|Intense\n",
      "מְרַחֶ֖פֶת  Move\n",
      "עַל־ Location\n"
     ]
    }
   ],
   "source": [
    "for w in L.d(T.nodeFromSection(('Genesis', 1)), 'word')[:30]:\n",
    "    \n",
    "    if not F.sem_domain.v(w):\n",
    "        continue\n",
    "    \n",
    "    print(T.text([w]), F.sem_domain.v(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14216"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find uncovered nouns\n",
    "\n",
    "uncovereds = []\n",
    "covereds = []\n",
    "\n",
    "for word in F.otype.s('word'):\n",
    "    \n",
    "    if F.pdp.v(word) in {'nmpr', 'subs'}:\n",
    "        \n",
    "        if not F.sem_domain.v(word):\n",
    "            uncovereds.append(word)\n",
    "            \n",
    "        else:\n",
    "            covereds.append(word)\n",
    "            \n",
    "len(uncovereds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1267"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncovered_lexs = collections.Counter(F.lex.v(w) for w in uncovereds)\n",
    "covered_lexs = collections.Counter(F.lex.v(w) for w in covereds)\n",
    "\n",
    "len(uncovered_lexs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PNH/', 2127),\n",
       " ('>JC/', 1332),\n",
       " ('CNH/', 643),\n",
       " ('NPC/', 621),\n",
       " ('<JN/', 567),\n",
       " ('JD/', 414),\n",
       " ('<FRJM/', 315),\n",
       " ('<T/', 296),\n",
       " ('<Y/', 288),\n",
       " ('LB/', 268)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncovered_lexs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('JHWH/', 6810),\n",
       " ('KL/', 5276),\n",
       " ('BN/', 4932),\n",
       " ('>LHJM/', 2599),\n",
       " ('MLK/', 2521),\n",
       " ('JFR>L/', 2500),\n",
       " ('>RY/', 2459),\n",
       " ('JWM/', 2233),\n",
       " ('BJT/', 2058),\n",
       " ('<M/', 1614)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covered_lexs.most_common(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
