{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Context Selection Development\n",
    "\n",
    "**TO DO**: Give full descriptions and walk-through for this NB.\n",
    "\n",
    "In this notebook, the `get_heads` function is built that will select nouns for semantic space construction in the next notebook. This function required a lot of testing and development. There are still potential oversights. If any are found, please let me know in Slack or at `codyakingham` via gmail.\n",
    "\n",
    "A new TF feature for the BHSA is exported in this NB called `heads`. It is an edge feature that connects phrase nodes with their head nouns (word nodes). This feature will be used to compare nouns with their verbs and with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 3.2.2\n",
      "Api reference : https://github.com/Dans-labs/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/Dans-labs/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Example data  : https://github.com/Dans-labs/text-fabric-data\n",
      "\n",
      "116 features found and 0 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.01s B book                 from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B chapter              from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s B verse                from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.06s B function             from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.11s B sp                   from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.11s B pdp                  from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.18s B mother               from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.19s B rela                 from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.19s B typ                  from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.11s B lex                  from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.11s B ls                   from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s Feature overview: 109 for nodes; 6 for edges; 1 configs; 7 computed\n",
      "  5.44s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "# First, I load the necesssary modules, data, and helper functions.\n",
    "import collections, random\n",
    "from tf.fabric import Fabric\n",
    "from functions.helpers import show_results, filter_results\n",
    "\n",
    "# load BHSA data into TF\n",
    "TF = Fabric(locations=['~/github/etcbc/bhsa/tf', '~/github/semantics/tf'], modules='c')\n",
    "api = TF.load('''\n",
    "                book chapter verse\n",
    "                function sp pdp mother\n",
    "                rela typ lex ls \n",
    "              ''')\n",
    "api.makeAvailableIn(globals()) # globalize TF methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Head Noun Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_phrs_type(phrase_atom, subphrases, diagnose=False):\n",
    "    '''\n",
    "    Return boolean on whether a phrase atom is an acceptable type.\n",
    "    Acceptable is either a noun phrase (NP) or\n",
    "    a prepositional phrase (PP) that is governed only by את.\n",
    "    '''\n",
    "\n",
    "    if F.typ.v(phrase_atom) == 'NP': # noun phrase\n",
    "        return True\n",
    "    \n",
    "    # for logic on this selection criteria, see [?]\n",
    "    prep_sp = sorted(sp for sp in subphrases # sorted sp with prepositions\n",
    "                         if 'prep' in set(F.pdp.v(w) for w in L.d(sp, 'word')))\n",
    "    phrase_type = prep_sp or (phrase_atom,)\n",
    "    preps = [w for w in L.d(phrase_type[0], 'word') \n",
    "                    if F.pdp.v(w) == 'prep']\n",
    "    \n",
    "    if F.typ.v(phrase_atom) == 'PP' and preps: # check for את\n",
    "        prep = preps[0]\n",
    "        if F.lex.v(prep) == '>T':\n",
    "            return True\n",
    "        else:\n",
    "            if diagnose:\n",
    "                print('>T not found...')\n",
    "            return False\n",
    "    else:\n",
    "        if diagnose:\n",
    "            print('neither NP or PP...')\n",
    "            print('phrase_type: ', phrase_type)\n",
    "        return False\n",
    "\n",
    "def get_quantified(abs_wnode, good_pdp, good_sp, diagnose=False):\n",
    "    '''\n",
    "    Extract the genitive noun in a construct chain with a quantifier.\n",
    "    The function simply returns the first substantive in the chain.\n",
    "    '''\n",
    "    \n",
    "    rectum = E.mother.t(abs_wnode) # get rectum subphrase\n",
    "    abs_phrase = L.d(L.u(abs_wnode, 'phrase')[0], 'word') # for phrase boundary\n",
    "    \n",
    "    if not rectum:\n",
    "        if diagnose:\n",
    "            print('no rectum found at word', abs_wnode)\n",
    "        return None  # abs not in norm. construct (e.g. w/ verbs)\n",
    "    \n",
    "    # get words and nouns in the rectum subphrase\n",
    "    r_words = L.d(rectum[0], 'word')\n",
    "    r_nouns = [w for w in r_words \n",
    "                   if F.sp.v(w) in good_sp\n",
    "                   and F.pdp.v(w) in good_pdp\n",
    "                   and w in abs_phrase]\n",
    "    if r_nouns:\n",
    "        return r_nouns[0] # return the first noun\n",
    "    else:\n",
    "        if diagnose:\n",
    "            print('no noun found for word', abs_wnode)\n",
    "        return None # no noun found, return nothing\n",
    "    \n",
    "def independent(phrase_atom, subphrases, heads_list, diagnose=False):\n",
    "    \n",
    "    '''\n",
    "    Checks phrase and subphrase relations for dependency relations.\n",
    "    Requires a list of previously analyzed head nouns.\n",
    "    This list is required to double check parallel (coordinate) relations. \n",
    "    '''\n",
    "    # exclude words in phrase_atoms with these relation features\n",
    "    omit_pa_rela = {'Appo', # apposition\n",
    "                    'Spec'} # specification\n",
    "    \n",
    "    # exclude words in subphrases with these relation features\n",
    "    omit_sp_rela = {'rec', # nomen rectum\n",
    "                    'adj', # adjunct \n",
    "                    'atr', # attributive\n",
    "                    'mod', # modifier\n",
    "                    'dem'} # demontrative\n",
    "    \n",
    "    parallels = {'par', 'Para'} # parallel i.e. coordination specification\n",
    "    omit_relas = omit_pa_rela | omit_sp_rela\n",
    "    phrase_units = list(subphrases) + [phrase_atom] # phrase atom & subphrase \n",
    "    relas = set(F.rela.v(obj) for obj in phrase_units) # phrase atom & subphrase relas\n",
    "    \n",
    "    if not relas & omit_relas and not parallels & relas: # good relas\n",
    "        return True\n",
    "    \n",
    "    elif not relas & omit_relas and parallels & relas: # check parallel relations\n",
    "        \n",
    "        # assemble acceptable phrase mothers from the already accepted head nouns\n",
    "        head_mothers = set(L.u(w, 'phrase_atom')[0] for w in heads_list)\n",
    "        head_mothers |= set(sp for w in heads_list\n",
    "                               for sp in L.u(w, 'subphrase'))\n",
    "        \n",
    "        for pu in phrase_units:\n",
    "            if F.rela.v(pu) in parallels:\n",
    "                mother = E.mother.f(pu)[0]\n",
    "                if mother in head_mothers:\n",
    "                    return True\n",
    "                else:\n",
    "                    if diagnose:\n",
    "                        print('False independence: ')\n",
    "                        print('phrase units', phrase_units)\n",
    "                        print('mothers', head_mothers)\n",
    "                        print('head mothers', head_mothers)\n",
    "                        print()\n",
    "                        \n",
    "                    return False\n",
    "                \n",
    "    else: # noun is not independent\n",
    "        if diagnose:\n",
    "            print('not an acceptable rela...')\n",
    "        return False  \n",
    "        \n",
    "    \n",
    "def get_heads(phrase, diagnose=False):\n",
    "    '''\n",
    "    Returns substantive head nouns, if there are any, from a phrase node.\n",
    "    \"substantive\" does not include prounouns.\n",
    "    \n",
    "    Based on a supplied phrase get phrase atom and subphrase features \n",
    "    and compare them against a group of sets.\n",
    "    Define those sets first. Then make the comparison.\n",
    "    \n",
    "    *Note*\n",
    "    Currently this function has been tested thoroughly only with phrases\n",
    "    that function as a subject or object within the clause. Theoretically\n",
    "    it should work with nearly any phrase type. But that has yet to be tested.\n",
    "    Also, the algorithm currently excludes pronouns.\n",
    "    '''\n",
    "    \n",
    "    good_sp = {'subs', 'nmpr', 'adjv'}\n",
    "    good_pdp = {'subs', 'nmpr'}\n",
    "        \n",
    "    heads = [] # nouns go here\n",
    "    phrase_words = L.d(phrase, 'word')\n",
    "        \n",
    "    for word in phrase_words:\n",
    "        \n",
    "        # get phrases's phrase atoms, subphrases, and subphrase relations\n",
    "        phrase_atom = L.u(word, 'phrase_atom')[0]\n",
    "        subphrases = L.u(word, 'subphrase') \n",
    "        sp_relas = set(F.rela.v(sp) for sp in subphrases)\n",
    "        \n",
    "        test_good = [F.pdp.v(word) in good_pdp, # is noun\n",
    "                     F.sp.v(word) in good_sp, # is noun\n",
    "                     good_phrs_type(phrase_atom, subphrases, diagnose), # is NP or PP with את\n",
    "                     independent(phrase_atom, subphrases, heads, diagnose) \n",
    "                    ] # is valid subphrase rela.\n",
    "        \n",
    "        # compare word/phrase features\n",
    "        if all(test_good):\n",
    "        \n",
    "            # handle quantifiers\n",
    "            quants = {'KL/', 'M<V/'}\n",
    "            if F.lex.v(word) in quants or F.ls.v(word) == 'card':\n",
    "                genitive_head = get_quantified(word, good_pdp, good_sp) # returns word node or None\n",
    "                if genitive_head:\n",
    "                    heads.append(genitive_head) # valid quantified noun found\n",
    "                else:\n",
    "                    continue # no noun found, skip it\n",
    "            else:\n",
    "                heads.append(word) # word is a head\n",
    "    \n",
    "        else:\n",
    "            if diagnose: \n",
    "                print(T.text([word]), word)\n",
    "                print('test_good', tuple(zip(test_good, ('pdp', 'sp', 'phr_typ', 'indep.'))))\n",
    "                print('subphrases', subphrases)\n",
    "                print('phrase_atom', phrase_atom)\n",
    "                print()\n",
    "            continue\n",
    "            \n",
    "    return heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject and Object Omissions\n",
    "\n",
    "A previous version of the valid preposition function only identified nouns from prepositional phrase atoms that spanned the entire functional phrase. That omits cases such as Josh 24:18 with constructs:\n",
    "> ('Joshua', 24, 18) <br>\n",
    "> 722643 אֶת־כָּל־הָעַמִּ֗ים וְאֶת־הָאֱמֹרִ֛י <br>\n",
    "\n",
    "The search below identifies phrases that begin with a preposition besides את and function as an object or subject (N.B. the prep. את can mark subjects in passive constructions). These are the cases which will be excluded by the new version of the function. A survey of these cases confirms that none of them contain nouns that are of interest: that is, none of these prepositions appear to grammaticaly mark a subject or object, but appear to be specifiers. \n",
    "\n",
    "For the old function, see `is_preposition_subj` in the [old version](https://github.com/codykingham/tfNotebooks/blob/master/4Q246_Participants/participant_functions/subjects.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 results\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 18px; line-height: 1\">1. Genesis 40:17</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 24px; line-height: 1\">וּבַסַּ֣ל הָֽעֶלְיֹו֔ן <span style=\"color: blue\">מִ</span><span style=\"color: blue\">כֹּ֛ל </span><span style=\"color: blue\">מַאֲכַ֥ל </span><span style=\"color: blue\">פַּרְעֹ֖ה </span><span style=\"color: blue\">מַעֲשֵׂ֣ה </span><span style=\"color: blue\">אֹפֶ֑ה </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 18px; line-height: 1\">2. Genesis 44:18</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 24px; line-height: 1\">כִּ֥י <span style=\"color: green\">כָמֹ֖וךָ </span>כְּפַרְעֹֽה׃ </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 18px; line-height: 1\">3. Genesis 47:26</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 24px; line-height: 1\">לְפַרְעֹ֖ה <span style=\"color: blue\">לַ</span><span style=\"color: blue\"></span><span style=\"color: blue\">חֹ֑מֶשׁ </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 18px; line-height: 1\">4. Exodus 4:9</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 24px; line-height: 1\">וְלָקַחְתָּ֙ <span style=\"color: green\">מִ</span><span style=\"color: green\">מֵּימֵ֣י </span><span style=\"color: green\">הַ</span><span style=\"color: green\">יְאֹ֔ר </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 18px; line-height: 1\">5. Exodus 17:5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 24px; line-height: 1\">וְקַ֥ח אִתְּךָ֖ <span style=\"color: blue\">מִ</span><span style=\"color: blue\">זִּקְנֵ֣י </span><span style=\"color: blue\">יִשְׂרָאֵ֑ל </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- \n",
      "\n",
      "results cut off at 5\n"
     ]
    }
   ],
   "source": [
    "targets = []\n",
    "\n",
    "for phrase in F.otype.s('phrase'):\n",
    "    \n",
    "    if F.function.v(phrase) not in {'Subj', 'Objc'}:\n",
    "        continue\n",
    "    \n",
    "    phrase_atoms = L.d(phrase, 'phrase_atom')\n",
    "    pa_lex = set(F.lex.v(w) for w in L.d(phrase_atoms[0], 'word'))\n",
    "        \n",
    "    if F.typ.v(phrase_atoms[0]) == 'PP' and '>T' not in pa_lex:\n",
    "        \n",
    "        targets.append((L.u(phrase, 'clause')[0], phrase, phrase_atoms[0]))\n",
    "        \n",
    "show_results(targets, limit=5, highlight=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further inspection of subject or object phrases that do not begin with את but contain את later on in the phrase shows that most of these are cases of adjectival specification, with a few parallel relationships reflected. In particular, it was important to be sure that את in the adjectival sense, especially where it has the sense of \"with\" rather than a grammatical one, would be excluded from the noun selector. Many of the finds by this search were more appositional than used in this \"with sense\". But a handful were found. 1 Chronicles 20:5, broken down below this search, confirms that את phrases in this adjectival sense are marked as `Spec` for specification. Thus, it is completely safe in the preposition parser function to take any את prepositional phrase. The acceptable subphrase relation set will then eliminate any match that is a specifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 results\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 18px; line-height: 1\">1. Genesis 6:10</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 24px; line-height: 1\">וַיֹּ֥ולֶד נֹ֖חַ <span style=\"color: blue\">שְׁלֹשָׁ֣ה </span><span style=\"color: blue\">בָנִ֑ים </span><span style=\"color: green\">אֶת־</span><span style=\"color: green\">שֵׁ֖ם </span><span style=\"color: green\">אֶת־</span><span style=\"color: green\">חָ֥ם </span><span style=\"color: green\">וְ</span><span style=\"color: green\">אֶת־</span><span style=\"color: green\">יָֽפֶת׃ </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 18px; line-height: 1\">2. Genesis 40:8</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 24px; line-height: 1\">וּ<span style=\"color: blue\">פֹתֵ֖ר </span>אֵ֣ין <span style=\"color: green\">אֹתֹ֑ו </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 18px; line-height: 1\">3. Genesis 41:15</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 24px; line-height: 1\">וּ<span style=\"color: blue\">פֹתֵ֖ר </span>אֵ֣ין <span style=\"color: green\">אֹתֹ֑ו </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 18px; line-height: 1\">4. Exodus 1:11</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 24px; line-height: 1\">וַיִּ֜בֶן <span style=\"color: blue\">עָרֵ֤י </span><span style=\"color: blue\">מִסְכְּנֹות֙ </span>לְפַרְעֹ֔ה <span style=\"color: green\">אֶת־</span><span style=\"color: green\">פִּתֹ֖ם </span><span style=\"color: green\">וְ</span><span style=\"color: green\">אֶת־</span><span style=\"color: green\">רַעַמְסֵֽס׃ </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 18px; line-height: 1\">5. Exodus 35:25</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: Times New Roman; font-size: 24px; line-height: 1\">וַיָּבִ֣יאוּ <span style=\"color: blue\">מַטְוֶ֗ה </span><span style=\"color: green\">אֶֽת־</span><span style=\"color: green\">הַ</span><span style=\"color: green\">תְּכֵ֨לֶת֙ </span><span style=\"color: green\">וְ</span><span style=\"color: green\">אֶת־</span><span style=\"color: green\">הָֽ</span><span style=\"color: green\">אַרְגָּמָ֔ן </span><span style=\"color: green\">אֶת־</span><span style=\"color: green\">תֹּולַ֥עַת </span><span style=\"color: green\">הַ</span><span style=\"color: green\">שָּׁנִ֖י </span><span style=\"color: green\">וְ</span><span style=\"color: green\">אֶת־</span><span style=\"color: green\">הַ</span><span style=\"color: green\">שֵּֽׁשׁ׃ </span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- \n",
      "\n",
      "results cut off at 5\n"
     ]
    }
   ],
   "source": [
    "targets = []\n",
    "\n",
    "for phrase in F.otype.s('phrase'):\n",
    "    \n",
    "    if F.function.v(phrase) not in {'Subj', 'Objc'}:\n",
    "        continue\n",
    "    \n",
    "    phrase_atoms = L.d(phrase, 'phrase_atom')\n",
    "    first_pa = phrase_atoms[0]\n",
    "    f_pa_lex = set(F.lex.v(w) for w in L.d(first_pa, 'word'))\n",
    "    \n",
    "    if '>T' in f_pa_lex:\n",
    "        continue\n",
    "    \n",
    "    for i in range(1, len(phrase_atoms)):\n",
    "        \n",
    "        other_pa = phrase_atoms[i]\n",
    "        pa_lex = set(F.lex.v(w) for w in L.d(other_pa, 'word'))\n",
    "        \n",
    "        if F.typ.v(other_pa) == 'PP' and '>T' in pa_lex:\n",
    "\n",
    "            targets.append((L.u(phrase, 'clause')[0], phrase, other_pa))\n",
    "            break\n",
    "        \n",
    "show_results(targets, limit=5, highlight=[1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen below, \"war *with Philistines*\" is marked as `Spec`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1158377\n",
      "מִלְחָמָ֖ה \n",
      "NA\n",
      "\n",
      "1158378\n",
      "אֶת־פְּלִשְׁתִּ֑ים \n",
      "Spec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (\"1_Chronicles\", 20, 5)\n",
    "for sp in (L.d(892236, 'phrase_atom')):\n",
    "    print(sp)\n",
    "    print(T.text(L.d(sp, 'word')))\n",
    "    print(F.rela.v(sp))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordering of Subphrases on a L.u call from Word Node\n",
    "\n",
    "This search demonstrates that subphrase nodes are indeed ordered by size when called from a word node, i.e. subphrases that contain less words receive smaller node numbers. If they are equal in size, either one might have the lower node number.\n",
    "\n",
    "It is indeed acceptable to select the first subphrase on a `L.u` call from a word for preposition testing. That is the subphrase which will be closest to the word level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_true = []\n",
    "true = 0\n",
    "no_sp = 0\n",
    "\n",
    "for word in F.otype.s('word'):\n",
    "    \n",
    "    subphrases = sorted(L.u(word, 'subphrase'))\n",
    "    \n",
    "    if not subphrases:\n",
    "        no_sp += 1\n",
    "        continue\n",
    "        \n",
    "    sp_len = sorted((len(L.d(sp, 'word')), sp) for sp in subphrases) # sort by word length\n",
    "    sp_check = [sp[1] for sp in sp_len] # iterate over sorted list and grab subphrase nodes\n",
    "    \n",
    "    if sp_check == subphrases: # check them\n",
    "        true += 1\n",
    "        \n",
    "    else:\n",
    "        not_true.append(word)\n",
    "\n",
    "len(not_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Many Phrases Does it Validate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'heads_found': 36406,\n",
       "         'pronoun_excluded': 5740,\n",
       "         'total': 54599,\n",
       "         'unknown': 12452})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_counts = collections.Counter()\n",
    "\n",
    "for phrase in F.otype.s('phrase'):\n",
    "    \n",
    "    if F.function.v(phrase) not in {'Subj', 'Objc'}:\n",
    "        continue\n",
    "        \n",
    "    heads = get_heads(phrase)\n",
    "    pdps = set(F.pdp.v(w) for w in L.d(phrase))\n",
    "    \n",
    "    phrase_counts['total'] += 1\n",
    "    \n",
    "    if heads:\n",
    "        phrase_counts['heads_found'] += 1\n",
    "        \n",
    "    elif pdps & {'prps', 'prde', 'prin'}:\n",
    "        phrase_counts['pronoun_excluded'] += 1\n",
    "    \n",
    "    elif pdps & {'intj'}:\n",
    "        phrase_counts['interjection_excluded']\n",
    "    \n",
    "    else:\n",
    "        phrase_counts['unknown'] += 1     \n",
    "        \n",
    "phrase_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push to TF as Edges\n",
    "\n",
    "Apply the function phrase atoms and phrases that serve as subjects or objects in the BHSA. The relationships will be pushed out to TF as an edge relation from a phrase to its head noun word nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 3.2.2\n",
      "Api reference : https://github.com/Dans-labs/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/Dans-labs/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Example data  : https://github.com/Dans-labs/text-fabric-data\n",
      "\n",
      "0 features found and 0 ignored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s Warp feature \"otype\" not found in\n",
      "\n",
      "  0.00s Warp feature \"oslots\" not found in\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Warp feature \"otext\" not found. Working without Text-API\n",
      "\n",
      "  0.00s Exporting 0 node and 1 edge and 0 config features to tf/c:\n",
      "   |     0.19s T heads                to tf/c\n",
      "  0.19s Exported 0 node features and 1 edge features and 0 config features to tf/c\n"
     ]
    }
   ],
   "source": [
    "push_new = False\n",
    "\n",
    "if push_new:\n",
    "\n",
    "    meta = {'': {'created_by': 'Cody Kingham',\n",
    "                 'coreData': 'BHSA',\n",
    "                 'coreVersion': 'c'\n",
    "                },\n",
    "            'heads' : {'source': 'see the notebooks at https://github.com/codykingham/semantics',\n",
    "                      'valueType': 'int',\n",
    "                      'edgeValues': False}\n",
    "           }\n",
    "\n",
    "    heads = {}\n",
    "\n",
    "    for phrase in F.otype.s('phrase'):\n",
    "\n",
    "        # only push features for these two types for now\n",
    "        if F.function.v(phrase) not in {'Subj', 'Objc'}:\n",
    "            continue\n",
    "\n",
    "        phrase_heads = get_heads(phrase)\n",
    "\n",
    "        if phrase_heads:\n",
    "            heads[phrase] = set(phrase_heads)\n",
    "\n",
    "        for phrase_atom in L.d(phrase, 'phrase_atom'):\n",
    "            phraseAt_heads = get_heads(phrase_atom)\n",
    "            if phraseAt_heads:\n",
    "                heads[phrase_atom] = set(phraseAt_heads)\n",
    "\n",
    "    new_edges = {'heads': heads}\n",
    "\n",
    "    saveTF = Fabric('tf/c')\n",
    "    saveTF.save(nodeFeatures={}, edgeFeatures=new_edges, metaData=meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Heads Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s loading features ...\n",
      "   |     0.30s T heads                from /Users/cody/github/semantics/tf/c\n",
      "  0.35s All additional features loaded - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "TF.load('heads', add=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ct = 0 \n",
    "# sample = []\n",
    "\n",
    "# # build a sample of interesting cases\n",
    "# for phrase in F.otype.s('phrase'):\n",
    "#     if E.heads.f(phrase) and len(L.d(phrase, 'word')) > len(E.heads.f(phrase)):\n",
    "        \n",
    "#         if len(E.heads.f(phrase)) > 2:\n",
    "#             sample.append(phrase)\n",
    "\n",
    "# print(len(sample), 'samples found\\n')    \n",
    "    \n",
    "# random.shuffle(sample)\n",
    "        \n",
    "# for phrase in sample:\n",
    "            \n",
    "#     clause = L.u(phrase, 'clause')[0]\n",
    "\n",
    "#     print(T.sectionFromNode(phrase))\n",
    "#     print(T.text(L.d(clause, 'word')))\n",
    "#     print(T.text(L.d(phrase, 'word')))\n",
    "#     print(E.heads.f(phrase))\n",
    "#     print(T.text(E.heads.f(phrase)))\n",
    "#     print()\n",
    "    \n",
    "#     if ct > 100:\n",
    "#         break\n",
    "        \n",
    "#     ct += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coordinate Nouns\n",
    "\n",
    "The function `get_heads` extracts head nouns for `Subj` and `Objc` phrases. But I have realized now that it was also a necessary step for extracting coordinate nouns. The new edge feature created by `get_heads`, `heads`, will also be used to compare coordinates with each other. So, for now, other phrase functions will be excluded from the analysis. The only thing left to do now is to build the semantic space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
