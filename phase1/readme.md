# Semantics, Phase 1

This repository contains notebooks used in the exploratory phase of this project. In this phase, three main goals were achieved. **First**, this project experimented with adapting the ETCBC's BHSA Hebrew data to a semantic space with syntactic selections. This first goal meant writing an algorithm, `get_heads`, which retrieved head nouns from a subject or object function phrase. This algorithm was necessary to separate the head nouns of a phrase from its modifying elements, allowing the noun itself to be analyzed and described. The goal also meant experimenting with different ways of creating basis elements. A basis element is another word for the cooccurrence surface form. Rather than simply observing surface cooccurrences for a verb, for instance, this project also looked at the target word's relation to the verb (subject or object) and the verb's stem (qal, hiphil, etc.). This entire first step required a lot of careful thinking about the structure of the ETCBC data; and it simultaneously suggested the possibility of a broader approach, by which relationships beyond mere subject or object could be considered; and it gave cause for a more robust head-noun selection technique, which has since been developed in the ETCBC [lingo repository](https://github.com/ETCBC/lingo/tree/master/heads). **Second**, the project applied methods from NLP and corpus linguistics for transforming the data. For the raw counts of coocurrences, two methods of adjustment were explored: pointwise mutual information (PMI) and the log likelihood G2 ("G test"). In addition, principle component analysis (PCA) was used to reduce multidimensional word vectors to two components which could be visualized on a scatter plot. **Third**, this project produced two semantic spaces using both methods: a PMI and log-likelihood space. The resulting spaces yielded a handfull of terms which were very well matched. For instance, livestock terms were frequently listed as each other's top most similar terms.

Also during Phase 1, a preliminary report was presented at the 2018 ETEN Conference in Copenhagen. This also resulted in a draft article (enclosed) which will be later submitted to the Hiphil journal for review. 

The phase 1 experiment has inspired numerous improvements for phase 2:
> 1) Implement a group of Python classes which contain all experiment parameters, and which can be easily modified and saved for back-referencing. This will allow for experiment parameters to be carefully tracked and recorded, and also for multiple kinds of results to be analyzed alongside one another. The classes contain BHSA target word and context selection measures, data transformation, and visualization methods. The BHSA target word and context selection methods break their tasks down into many smaller methods that can easily be exchanged or supplemented with inheriting classes.<br>
> 2) A more robust method of head selection was designed and saved to the ETCBC lingo repository (see link above). This method can handle any phrase type rather than just subject or object phrases.<br>
> 3) Semantic spaces will be added on top of each other to enhance their accuracy. Specifically, phase 2 will attempt to use verb and noun vectors as cooccurrence bases instead of individual basis elements. This requires in effect two runs: a rudimentary space for both verbs and nouns and a compositional space that uses the rudimentary spaces in a second run. 

## Contents
The repository consists of these primary analysis notebooks:

1) word2vec Experiment - An experiment that applies word2vec yields a set similarity sets. These sets are interesting but contain mixed results, such as the combination of numbers with animals (due to a high frequency of co-occurrence). However, these sets are useful for notebook 2.
2) Context Selection Discovery - This NB explores which colexemes should be selected on the basis of the noun's status in the clause. E.g. if the noun is in a direct object function phrase, the verb of the predicate phrase is valuable and should be considered. The similar words generated in the word2vec experiment (1) are used to identify other desirable traits that should be considered in the selection process.
3) Context Selection Development - This NB develops the functions that can extract the needed relations identified in the context selection discovery NB.
4) Semantic Space Construction - The semantic space is constructed using the context selection functions. Words are clustered into similarity sets. The results are exported to a text-fabric data representation stored on lexeme nodes in the BHSA dataset.

The repository also contains text-fabric data files (under tf) which were created with the project's version of `get_heads`. 

Under data is a set of the top 50 most common terms in the semantic spaces with all other target words ranked from most to least similar. The simmilarity score is included: the closer to 1 the score, the more similar are the terms. 

The images directory contains assorted images used or produced by the project including images of the PCA-transformed semantic spaces.
